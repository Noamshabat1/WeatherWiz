{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37e1192c-4b79-4ac2-a867-5df33a97c54a",
   "metadata": {},
   "source": [
    "# Weather Wiz: An AI-Based Weather Forecasting Project\n",
    "\n",
    "Final project for the course Machine Learning in Earth and Environmental Sciences (70938) by Prof. Efrat Morin  \n",
    "Authors:  \n",
    "Noam Shabat – Noam.Shabat@mail.huji.ac.il  \n",
    "Tomer Vagenfeld – Tomer.vagenfeld@mail.huji.ac.il\n",
    "\n",
    "---\n",
    "\n",
    "## Abstract\n",
    "Weather Wiz is an AI-based project to forecast ground temperature using 25 years of multi-station meteorological data provided by the Israel Meteorological Service (IMS). This project explores a range of machine learning approaches—from traditional regularized regression models to deep learning architectures such as Long Short-Term Memory networks (LSTM) and Graph Neural Networks (GNN). By capturing both temporal trends and spatial correlations, the system produces short-term forecasts.\n",
    "\n",
    "In this paper, we describe the problem formulation, data characteristics, preprocessing and exploratory analyses, modeling methodologies, and experimental results, and discuss insights obtained in the project.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Accurate weather forecasting is important in many fields, including agriculture, energy, transportation, and emergency management. The Weather Wiz project focuses on predicting ground temperature (TG) using a dataset spanning 25 years of high-resolution weather measurements. Given the complex nature of meteorological phenomena—which are influenced by humidity, wind patterns, and precipitation—this project employs a combination of linear models, ensemble methods, and deep learning techniques. In particular, the use of Graph Neural Networks enables the integration of spatial relationships among weather stations, thereby improving prediction performance.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Data Description\n",
    "\n",
    "### 2.1 Data Source, Station Map, and Nature\n",
    "The dataset is obtained from the [Israel Meteorological Service (IMS)](https://ims.gov.il), which records weather data from multiple stations across Israel. Observations are recorded every 10 minutes and subsequently aggregated into hourly or daily summaries. This extensive temporal coverage and multi-station design provide a detailed view of regional weather dynamics.\n",
    "\n",
    "**Station Map:**  \n",
    "The station locations were extracted directly from the IMS API, and the map was created using ArcGIS.  \n",
    "![Station Map](images/station_map.png)  \n",
    "*Figure 2.1 – Station Map: This map displays the geographic locations of the IMS stations across Israel.*\n",
    "\n",
    "### 2.2 Data Extraction from the IMS API\n",
    "Data extraction was performed by interfacing with the IMS API, which provides secure access to detailed meteorological data in JSON format. By specifying station identifiers and a defined date range, the system automatically queries and aggregates data on a monthly basis to address API limitations. The extracted data is then converted into structured tabular formats for further processing and analysis, ensuring complete coverage of the 25-year period.\n",
    "\n",
    "### 2.3 Features and Target Variable\n",
    "- **Features:**\n",
    "  - **Meteorological Variables:** Measurements such as relative humidity (RH), wind speed (WS), wind direction (WD), and rainfall (Rain).\n",
    "  - **Engineered Features:**  \n",
    "    - *Temporal Attributes:* Hour of day along with sine and cosine transformations to capture cyclical patterns.  \n",
    "    - *Wind Vectors:* Components derived from wind speed and wind direction that quantify directional wind influence.\n",
    "    - * Spatial Features:* Longitude and Latitude of eacg station\n",
    "- **Target:**  \n",
    "  - **Ground Temperature (TG):** The variable to be predicted, representing the ground-level temperature.\n",
    "\n",
    "### 2.4 Data Visualization\n",
    "- **Feature Explanation:**  \n",
    "  An illustration of the features and their descriptions is provided below.  \n",
    "  ![Features explaination](images/features_explain.png)  \n",
    "  *Figure 2.2 – Features Explanation: This diagram details the various meteorological and engineered features used in the project. Image provided by the IMS.*\n",
    "\n",
    "- **Ground Temperature Analysis:**  \n",
    "  The distribution and time series of ground temperature (TG) are visualized as follows:  \n",
    "  ![TG Distribution](images/TG_distribution.png)  \n",
    "  *Figure 2.3 – TG Distribution: This histogram shows the distribution of ground temperature values across the dataset.*  \n",
    "\n",
    "  ![TG Series Full](images/TG_series%20full.png)  \n",
    "  *Figure 2.4 – TG Series (Full): This plot shows the complete time series of ground temperature measurements over the 25-year period.*  \n",
    "\n",
    "  ![TG Series](images/TG_series.png)  \n",
    "  *Figure 2.5 – TG Series: This plot provides a detailed view of short-term fluctuations in ground temperature.*\n",
    "\n",
    "### 2.5 Sample Data\n",
    "Below is a table presenting sample data extracted from the IMS API, including the major features used later in training. *Reminder that the data captured by the meteorological station is taken in 10-minute intervals.*\n",
    "\n",
    "| Region ID | Station Name           | station_id | datetime                       | RH | Rain | TG   | WD  | WS  | Latitude | Longitude | Min Date                       | Max Date                       | Date Range (days approx.) |\n",
    "|-----------|------------------------|------------|--------------------------------|----|------|------|-----|-----|----------|-----------|--------------------------------|--------------------------------|---------------------------|\n",
    "| 8         | TAVOR KADOORIE        | 13         | 2010-11-30T04:00:00+02:00      | 25 | 0    | 9.1  | 306 | 1.8 | 32.7053  | 35.4069   | 2000-01-31T00:00:00+02:00      | 2024-09-30T23:50:00+03:00      | 9000                      |\n",
    "| 9         | ZEMAH                 | 8          | 2022-03-31T19:50:00+03:00      | 77 | 0    | 13.8 | 201 | 0.1 | 32.7024  | 35.5839   | 2000-01-31T00:00:00+02:00      | 2024-10-31T23:50:00+02:00      | 9000                      |\n",
    "| 8         | TAVOR KADOORIE        | 13         | 2017-07-31T13:20:00+03:00      | 38 | 0    | 52.8 | 302 | 2.9 | 32.7053  | 35.4069   | 2000-01-31T00:00:00+02:00      | 2024-09-30T23:50:00+03:00      | 9000                      |\n",
    "| 9         | GILGAL                | 30         | 2018-06-30T21:10:00+03:00      | 45 | 0    | 27.9 | 254 | 0.8 | 31.9973  | 35.4509   | 2000-01-31T00:00:00+02:00      | 2024-12-31T23:50:00+02:00      | 9000                      |\n",
    "| 9         | GILGAL                | 30         | 2022-04-30T09:20:00+03:00      | 27 | 0    | 40.4 | 121 | 0.8 | 31.9973  | 35.4509   | 2000-01-31T00:00:00+02:00      | 2024-12-31T23:50:00+02:00      | 9000                      |\n",
    "| 9         | GILGAL                | 30         | 2001-07-31T12:10:00+03:00      | 39 | 0    | 51.3 | 147 | 1.8 | 31.9973  | 35.4509   | 2000-01-31T00:00:00+02:00      | 2024-12-31T23:50:00+02:00      | 9000                      |\n",
    "| 8         | MEROM GOLAN PICMAN    | 10         | 2013-04-30T18:30:00+03:00      | 38 | 0    | 18.3 | 306 | 2.5 | 33.1288  | 35.8045   | 2000-01-31T00:00:00+02:00      | 2024-12-31T23:50:00+02:00      | 9000                      |\n",
    "| 8         | TAVOR KADOORIE        | 13         | 2011-12-31T06:20:00+02:00      | 87 | 0    | 6.3  | 320 | 1.1 | 32.7053  | 35.4069   | 2000-01-31T00:00:00+02:00      | 2024-09-30T23:50:00+03:00      | 9000                      |\n",
    "| 10        | YOTVATA               | 36         | 2012-05-31T09:40:00+03:00      | 31 | 0    | 38.8 | 6   | 2.9 | 29.8851  | 35.0771   | 2000-02-29T00:00:00+02:00      | 2024-12-31T23:50:00+02:00      | 9000                      |\n",
    "| 9         | GILGAL                | 30         | 2011-02-28T05:00:00+02:00      | 80 | 0    | 8.2  | 232 | 1.6 | 31.9973  | 35.4509   | 2000-01-31T00:00:00+02:00      | 2024-12-31T23:50:00+02:00      | 9000                      |\n",
    "\n",
    "*Table 2.1 – Sample Data: This table presents 10 rows from the dataset extracted via the IMS API, including station identifiers, timestamps, key meteorological measurements, and additional metadata (e.g., latitude, longitude, date range).*\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Data Preprocessing and Exploratory Data Analysis\n",
    "\n",
    "### 3.1 Data Cleaning and Preparation\n",
    "Ensuring high data quality is essential for reliable model training. Our data cleaning process included:\n",
    "- **Removal of Unrealistic Values:**  \n",
    "  Data entries were filtered to enforce physical plausibility (e.g., ensuring \\(0 \\leq \\text{RH} \\leq 100\\) and \\(-15 \\leq \\text{TG} \\leq 50\\)). Outliers and values outside known meteorological ranges were removed.\n",
    "- **Handling Missing Values:**  \n",
    "  Missing or null values in critical variables were identified and addressed. Depending on the variable, rows with missing data were either dropped or imputed.\n",
    "- **Datetime Conversion and Sorting:**  \n",
    "  Timestamp strings were converted into datetime objects to ensure proper chronological ordering, which is critical for time series analysis.\n",
    "- **Normalization and Scaling:**  \n",
    "  To ensure that all input features contribute equally during model training, the data was scaled using a MinMaxScaler. The scaling transformation is defined as:\n",
    "  $$\n",
    "  x' = \\frac{x - x_{\\min}}{x_{\\max} - x_{\\min}},\n",
    "  $$\n",
    "  where \\(x\\) is the original feature value, and \\(x_{\\min}\\) and \\(x_{\\max}\\) are the minimum and maximum values of that feature. This process is crucial for models sensitive to input scale, as it improves convergence speed and stability.\n",
    "- **Feature Engineering:**  \n",
    "  Additional features were computed to capture underlying patterns in the data.\n",
    "\n",
    "#### Mathematical Operations in Feature Engineering\n",
    "To represent cyclical and directional information effectively, the following mathematical transformations were applied:\n",
    "- **Cyclical Time Features:**  \n",
    "  The hour of the day was transformed using sine and cosine functions:\n",
    "  $$\n",
    "  \\text{hour}_{\\sin} = \\sin\\left(\\frac{2\\pi \\times \\text{hour}}{24}\\right),\n",
    "  $$\n",
    "  $$\n",
    "  \\text{hour}_{\\cos} = \\cos\\left(\\frac{2\\pi \\times \\text{hour}}{24}\\right).\n",
    "  $$\n",
    "  These transformations allow the model to capture the periodic nature of time.\n",
    "  \n",
    "- **Wind Vector Components:**  \n",
    "  Wind speed (\\(WS\\)) and wind direction (\\(WD\\)) were converted from polar to Cartesian coordinates:\n",
    "  $$\n",
    "  wind_x = WS \\times \\cos\\left(\\frac{\\pi \\times WD}{180}\\right),\n",
    "  $$\n",
    "  $$\n",
    "  wind_y = WS \\times \\sin\\left(\\frac{\\pi \\times WD}{180}\\right).\n",
    "  $$\n",
    "  This conversion provides a richer representation of wind dynamics.\n",
    "\n",
    "### 3.2 Exploratory Data Analysis (EDA)\n",
    "EDA was conducted to understand the structure and variability of the dataset:\n",
    "- **Distribution Analysis:**  \n",
    "  Histograms and density plots illustrate the distributions of temperature, humidity, and rainfall.\n",
    "- **Time Series Visualization:**  \n",
    "  Time series plots highlight both short-term fluctuations and long-term trends in ground temperature.\n",
    "- **Correlation Analysis:**  \n",
    "  Correlation matrices help identify significant relationships among features, guiding further feature engineering.\n",
    "- **Missing Value Patterns:**  \n",
    "  Visualizations of missing data assess data quality and inform imputation strategies.\n",
    "\n",
    "Additional visualizations, such as zoomed-in seasonal trends and scatter plots comparing multiple variables, provided further insights that informed model development.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Methodology\n",
    "\n",
    "### 4.1 Modeling Strategy\n",
    "A multi-model strategy was implemented to address different aspects of the data:\n",
    "- **Lasso Regression:**  \n",
    "  Used as a baseline model for its simplicity and interpretability. Its L1 regularization promotes sparsity, thereby highlighting the most relevant features.\n",
    "- **Random Forest:**  \n",
    "  An ensemble method that captures complex nonlinear interactions among features. Its robustness makes it well-suited for high-dimensional data.\n",
    "- **Long Short-Term Memory (LSTM) Networks:**  \n",
    "  LSTMs are effective at capturing long-term dependencies in sequential data, making them ideal for modeling temporal dynamics.\n",
    "- **Graph Neural Networks (GNN):**  \n",
    "  GNNs integrate spatial information by treating weather stations as nodes in a graph, leveraging both spatial and temporal correlations to capture localized weather patterns.\n",
    "\n",
    "### 4.2 Explanation of Graph Neural Networks (GNN)\n",
    "Graph Neural Networks (GNNs) are neural architectures designed to operate on graph-structured data. Unlike traditional neural networks that work on fixed grids or sequences, GNNs handle irregular, interconnected data. In a GNN, each node (e.g., a weather station) gathers information from its neighboring nodes through a process called message passing. During this process, each node updates its representation by combining its features with those of its neighbors via learnable functions. This enables the network to capture both local interactions and the overall structure of the graph, making it particularly effective for modeling spatial relationships. The flexibility to include additional node or edge features further enhances the network’s capacity to represent complex spatial dependencies.\n",
    "\n",
    "**Unique Edge Technique Attempt:**  \n",
    "We initially experimented with a \"unique edge\" technique intended to incorporate additional station-to-station relationships or domain-specific spatial distances. However, this approach exceeded our available RAM resources, preventing us from fully leveraging it. We believe that, with sufficient computational resources, implementing the unique edge technique would allow the GNN to model more granular connectivity between stations, potentially improving its performance in capturing spatial dependencies.\n",
    "\n",
    "### 4.3 Training and Optimization\n",
    "\n",
    "#### 4.3.1 Optimization Functions in the Models\n",
    "Each model uses an optimization strategy tailored to its structure:\n",
    "- **LSTM Models – Adam Optimizer:**  \n",
    "  The LSTM networks are trained using the Adam algorithm, which adapts the learning rate for each parameter based on estimates of the first and second moments of the gradients:\n",
    "  $$\n",
    "  m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t,\n",
    "  $$\n",
    "  $$\n",
    "  v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2,\n",
    "  $$\n",
    "  $$\n",
    "  \\hat{m}_t = \\frac{m_t}{1-\\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1-\\beta_2^t},\n",
    "  $$\n",
    "  $$\n",
    "  w_{t+1} = w_t - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon},\n",
    "  $$\n",
    "  where \\(g_t\\) is the gradient at time \\(t\\) and \\(\\alpha\\), \\(\\beta_1\\), \\(\\beta_2\\), and \\(\\epsilon\\) are hyperparameters.\n",
    "  \n",
    "- **GNN Models – AdamW Optimizer:**  \n",
    "  The GNN models use the AdamW optimizer, which decouples weight decay from the gradient update to improve generalization:\n",
    "  $$\n",
    "  w_{t+1} = w_t - \\alpha \\left( \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} + \\lambda w_t \\right),\n",
    "  $$\n",
    "  where \\(\\lambda\\) is the weight decay coefficient.\n",
    "  \n",
    "- **Lasso Regression and Random Forest:**  \n",
    "  Lasso Regression uses coordinate descent to solve:\n",
    "  $$\n",
    "  \\min_{w} \\; \\|y - Xw\\|_2^2 + \\alpha \\|w\\|_1,\n",
    "  $$\n",
    "  promoting sparsity in \\(w\\). Random Forests build an ensemble of decision trees using recursive partitioning and impurity minimization (e.g., minimizing mean squared error) to generate predictions.\n",
    "\n",
    "#### 4.3.2 Hyperparameter Tuning\n",
    "Hyperparameters such as the number of hidden units, dropout rates, learning rates, and the number of epochs were tuned within predefined ranges (e.g., LSTM hidden units between 32 and 128, GNN dimensions between 32 and 256) to ensure efficient convergence and robust performance.\n",
    "\n",
    "### 4.4 Cross-Validation and Data Folds\n",
    "To evaluate model performance reliably, a custom time series cross-validation strategy was employed:\n",
    "- **Chronological Splitting:**  \n",
    "  The dataset is divided into sequential folds, where each fold consists of a training set (earlier time periods) and a test set (later time periods), reflecting a realistic forecasting scenario.\n",
    "- **Multiple Folds:**  \n",
    "  Evaluating across several folds provides insights into the stability and consistency of the model’s predictions over different time intervals.\n",
    "- **Avoiding Data Leakage:**  \n",
    "  Preserving the temporal order prevents future information from influencing the training process, ensuring an unbiased evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Experimental Results\n",
    "\n",
    "### 5.1 Evaluation Metrics\n",
    "Models were assessed using the following metrics:\n",
    "- **Mean Absolute Error (MAE):** The average absolute difference between predicted and actual temperatures.\n",
    "- **Mean Squared Error (MSE):** The average squared difference between predicted and actual temperatures.\n",
    "- **Coefficient of Determination (R² Score):** The proportion of variance in the target variable explained by the model.\n",
    "- **Median Absolute Percentage Error (MdAPE):** A relative measure of prediction accuracy.\n",
    "\n",
    "### 5.2 Visualizations of Model Performance\n",
    "Several plots were generated to illustrate model performance:\n",
    "- **Random Forest Results:**  \n",
    "  ![Random Forest](images/random_forest.png)  \n",
    "  *Figure 5.1 – Random Forest Results: This figure illustrates the performance of the Random Forest model in predicting ground temperature.*\n",
    "- **Lasso Regression Results:**  \n",
    "  ![Lasso](images/lasso.png)  \n",
    "  *Figure 5.2 – Lasso Regression Results: This figure shows the performance of the Lasso Regression model as a baseline.*\n",
    "- **LSTM Performance:**  \n",
    "  ![LSTM](images/lstm.png)  \n",
    "  *Figure 5.3 – LSTM Performance: This figure depicts the LSTM model's ability to capture temporal dependencies in the data.*\n",
    "- **Ground Temperature Predictions:**  \n",
    "  ![TG Predictions](images/TG_pred.png)  \n",
    "  *Figure 5.4 – Ground Temperature Predictions: This plot compares the actual and forecasted ground temperature values over time.*  \n",
    "  ![TG Predictions 2](images/TG_pred_2.png)  \n",
    "  *Figure 5.5 – Ground Temperature Predictions (Alternate View): This plot provides an alternative visualization of the forecasted ground temperature.*\n",
    "- **Loss Curve for GNN:**  \n",
    "  ![Loss Curve](images/loss.png)  \n",
    "  *Figure 5.6 – Loss Curve for GNN: This graph shows the training and validation loss trends during GNN model training.*\n",
    "- **Evaluation Metrics:**  \n",
    "  ![MAE](images/mae.png)  \n",
    "  *Figure 5.7 – MAE: This figure displays the Mean Absolute Error for the models across the test folds.*  \n",
    "  ![MSE](images/mse.png)  \n",
    "  *Figure 5.8 – MSE: This figure shows the Mean Squared Error for the models.*  \n",
    "  ![R²](images/r2.png)  \n",
    "  *Figure 5.9 – R² Score: This figure illustrates the R² scores, indicating the proportion of variance explained by the models.*\n",
    "- **Cross-Validation Summary:**  \n",
    "  ![Cross-Validation Summary](images/cross.png)  \n",
    "  *Figure 5.10 – Cross-Validation Summary: This bar chart summarizes the performance of different models across multiple cross-validation folds.*\n",
    "\n",
    "\n",
    "### 5.3 Discussion of Results\n",
    "\n",
    "Results indicate that:\n",
    "- **Lasso Regression**\n",
    "\n",
    "\n",
    "  As a strart it is a strong baseline. Although its performance was modest compared to more complex models, its simplicity and transparency are useful and it is useful benchmark.\n",
    " \n",
    "- **Random Forest** \n",
    "\n",
    "\n",
    "    effectively captures nonlinear interactions among the features. As we learned in class, this ensemble method benefits from averaging multiple decision trees, which helps reduce variance.\n",
    "\n",
    "    However, the predictions from Random Forest can still exhibit higher variance, particularly when faced with abrupt changes or rare events in the data. The model’s performance varied slightly across different cross-validation folds, indicating sensitivity to the particular splits of the time series data. \n",
    "\n",
    "- **LSTM Networks**\n",
    "\n",
    "  It excels in modeling temporal dependencies, particularly for short-term forecasts like our goal. The LSTM was able to capture the overall trend and seasonality of the ground temperature effectively. However, while it performed well overall, it struggled to accurately predict extreme values.\n",
    "\n",
    "  This is a common limitation of LSTM models: because they are trained to minimize a loss function like MSE, they tend to produce smoothed predictions that capture the central tendency of the data. Extreme values, which occur infrequently (as can seen in the histogram on section 2), may be underrepresented during training and thus are often averaged out, leading to a lower sensitivity to outlier events. In future project we should concider oversampling extreme values in order to mitigate this problem as we did in Excercise 3 and 4. \n",
    "\n",
    "- **GNN Models**\n",
    "\n",
    "The GNN was used in order to enhance the prediction accuracy by incorporating spatial correlations. By treating each weather station as a node in a graph, the GNN was able to leverage the connectivity between nearby stations, which is especially beneficial in regions with a dense network of stations. This spatial context allowed the model to adjust predictions based on local anomalies and environmental similarities.\n",
    "\n",
    "While the current implementation did not use the \"unique edge\" technique due to memory constraints. We anticipate that a fully optimized version incorporating unique edges would further improve the model’s ability to capture fine-grained spatial relationships.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### 6. Discussion\n",
    "- Data quality and model performance\n",
    " \n",
    "    First, the data quality varied significantly between stations. many stations provided only sporadic recordings, often only a few days worth of data at a time, and some lacked critical sensors (for example, barometric pressure). These issues posed challenges for us in training the data.\n",
    "    The discontinuity and missing measurements introduced noise and limited the range of features available for learning. Despite these limitations, our models were still able to predict ground temperature with a reasonable degree of precision based on the features available. That's why we used all the data from all the stations in the same time-series while the sptaial coordinates used to differentiate between them. It allowed us to train a big model with around 0.5 milion rows of data.  \n",
    "\n",
    "    As we mentioned before, the LSTM networks effectively captured the overall temporal trends, but they tended to smooth over extreme temperature values. This behavior is consistent with the tendency of LSTMs to focus on the dominant patterns in the data, often at the expense of rare or outlier events.\n",
    "\n",
    "    Additionally, the predictive accuracy for short-term forecasts could likely be improved by incorporating additional meteorological features, such as barometric pressure and radiation, which were not available in our dataset for most stations. These variables could have significant influence on surface temperature variations and prediction, eventualy to capture more nuanced and specific weather events.\n",
    "\n",
    "- Challenges\n",
    " \n",
    "    The challenges we encountered in this project highlights the critical importance of both data quality and feature completeness in weather forecasting tasks and in ML projects in general. Our dataset was not entirely clean or continuous. to solve this cross-validation graph to decrease the affect overfitting to a specific time frames, and later use it as an ensable model.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Conclusion\n",
    "\n",
    "Despite the challenges, our results confirm that ground temperature can be predicted with a reasonable level of precision using the available features. However, the performance ceiling for short-term forecasts appears to be limited by the absence of crucial additional features such as barometric pressure and radiation, which are vital for capturing transient weather phenomena. \n",
    "\n",
    "---\n",
    "\n",
    "## Acknowledgments\n",
    "This project was made possible by the data provided by the [Israel Meteorological Service (IMS)](https://ims.gov.il).\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3722d350-d43b-4800-980c-9cfda3ddd630",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3758a102-e18d-4289-92f2-f2d8f8269ae6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
