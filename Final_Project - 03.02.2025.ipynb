{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IN2QmRpRaaZt"
      },
      "source": [
        "#Final Project - Weather Wiz\n",
        "\n",
        "### Overview\n",
        "  Weather Wiz is a cutting-edge machine learning project designed to predict weather patterns, specifically temperature and radiation, using 25 years of historical data collected from meteorological stations across Israel by the Israel Meteorological Service (IMS). The data was sampled every 10 minutes daily and later aggregated into a daily template.\n",
        "\n",
        "\n",
        "#TODOs:\n",
        "- לסדר את המידע ולהוסיף את כל הפיצרים של התחנות שחסר (קורדינטות ועוד..)\n",
        "- להכניס את ה EDA לקוד ולבדוק שהועא עובד\n",
        "- לסדק את ה data prep אחרי התיקון של המידע\n",
        "- להמשיך לסדר את ההייפר פרמטרים\n",
        "- להכניס מצגת להצגה וגם קובץ הגשה\n",
        "- לשמור תמונות רלוונטיות לתרגיל כדאי להעלות לגיט\n",
        "- לסדר את התצוגה של הגרפים\n",
        "- לסדר את הקוד מחדש לפני הנושאים הרלוונטים"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oT9VjYMeY7sM"
      },
      "source": [
        "# Connect to Google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYWYyknnY7Jb",
        "outputId": "e8c0c3f1-c623-4226-d18c-1fac93bafe00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Connect to Google drive\n",
        "# -------------------------------------------------------------------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLwA-XUbaRIw"
      },
      "source": [
        "# Importing Needed Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiBvwD-vtkFe",
        "outputId": "3c3cabf0-acb1-4c6f-8baa-3179fe2742eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.1.0+cpu.html\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.11/dist-packages (2.1.2+pt21cpu)\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.11/dist-packages (0.6.18+pt21cpu)\n",
            "Requirement already satisfied: torch-cluster in /usr/local/lib/python3.11/dist-packages (1.6.3+pt21cpu)\n",
            "Requirement already satisfied: torch-spline-conv in /usr/local/lib/python3.11/dist-packages (1.2.2+pt21cpu)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.11)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2024.12.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.13.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2024.12.14)\n",
            "All imports are successful!\n"
          ]
        }
      ],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Importing Needed Packages\n",
        "# -------------------------------------------------------------------\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import math\n",
        "\n",
        "# TensorFlow\n",
        "import tensorflow as tf\n",
        "\n",
        "# Scikit-learn for Machine Learning\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "#if theres a probelm with the import.\n",
        "!pip install torch-geometric torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.1.0+cpu.html\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "# Miscellaneous\n",
        "from collections import Counter\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Confirmation of successful imports\n",
        "# -------------------------------------------------------------------\n",
        "print(\"All imports are successful!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39SBkRIo7N8u",
        "outputId": "fee01e73-aa76-4051-8352-352f209955e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Set Seeds for Reproducibility & Device Management\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wk0Yoks2P58p"
      },
      "source": [
        "# --------------------------- Helper Functions ---------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "k0QULukAP5nh"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Helper Functions for Evaluation & Plotting\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "def median_absolute_percentage_error(y_true, y_pred, delta=1e-3, epsilon=1e-6):\n",
        "    \"\"\"\n",
        "    Compute a modified median absolute percentage error (MdAPE) that ignores\n",
        "    samples with |y_true| less than delta.\n",
        "\n",
        "    Parameters:\n",
        "        y_true (array-like): True target values.\n",
        "        y_pred (array-like): Predicted target values.\n",
        "        delta (float): Threshold below which a true value is considered too close to zero.\n",
        "        epsilon (float): Small constant to prevent division by zero.\n",
        "\n",
        "    Returns:\n",
        "        float: The median absolute percentage error calculated only on samples with |y_true| > delta.\n",
        "    \"\"\"\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    # Create a mask for valid samples\n",
        "    mask = np.abs(y_true) > delta\n",
        "    if np.sum(mask) == 0:\n",
        "        return np.nan  # or 0, depending on your preference\n",
        "    relative_errors = np.abs((y_true[mask] - y_pred[mask]) / np.maximum(np.abs(y_true[mask]), epsilon)) * 100\n",
        "    return np.median(relative_errors)\n",
        "\n",
        "\n",
        "def evaluate_model(model_name, y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Evaluate a model's performance using various metrics.\n",
        "\n",
        "    Parameters:\n",
        "        model_name (str): Name of the model.\n",
        "        y_true (array-like): True target values.\n",
        "        y_pred (array-like): Predicted target values.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (mae, mse, r2, mdape)\n",
        "    \"\"\"\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    r2  = r2_score(y_true, y_pred)\n",
        "    mdape = median_absolute_percentage_error(y_true, y_pred)\n",
        "\n",
        "    print(f\"{model_name} Performance:\")\n",
        "    print(f\"  MAE: {mae:.4f}\")\n",
        "    print(f\"  MSE: {mse:.4f}\")\n",
        "    print(f\"  R2: {r2:.4f}\")\n",
        "    print(f\"  MdAPE: {mdape:.4f}%\\n\")\n",
        "    return mae, mse, r2, mdape\n",
        "\n",
        "\n",
        "def train_and_evaluate(model, model_name, train_X, test_X, train_y, test_y):\n",
        "    \"\"\"\n",
        "    Train a scikit-learn model and evaluate its performance.\n",
        "\n",
        "    This function fits the given model on the training data, makes predictions\n",
        "    on the test data, evaluates the predictions, and plots actual vs. predicted values.\n",
        "\n",
        "    Parameters:\n",
        "        model: scikit-learn model instance.\n",
        "        model_name (str): Name of the model (for printing and plotting).\n",
        "        train_X: Training feature data.\n",
        "        test_X: Testing feature data.\n",
        "        train_y: Training target values.\n",
        "        test_y: Testing target values.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (mae, mse, r2, MdAPE, predictions)\n",
        "    \"\"\"\n",
        "    # Train the model\n",
        "    model.fit(train_X, train_y)\n",
        "    predictions = model.predict(test_X)\n",
        "    predictions = np.round(predictions, 2)\n",
        "\n",
        "    # Evaluate the model\n",
        "    mae, mse, r2, mdape = evaluate_model(model_name, test_y, predictions)\n",
        "\n",
        "    # Plot Actual vs. Predicted values\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(test_y, predictions, alpha=0.6, label=\"Predicted vs Actual\")\n",
        "    plt.plot([min(test_y), max(test_y)], [min(test_y), max(test_y)], 'r--', label=\"Perfect Fit\")\n",
        "    plt.xlabel(\"Actual Values\")\n",
        "    plt.ylabel(\"Predicted Values\")\n",
        "    plt.title(f\"{model_name} - Actual vs Predicted\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return mae, mse, r2, mdape, predictions\n",
        "\n",
        "\n",
        "def plot_series(test_obs, test_preds, model_name, fold_idx):\n",
        "    test_obs.index = pd.to_datetime(test_obs.index)\n",
        "    test_preds = pd.Series(test_preds, index=test_obs.index)\n",
        "    fig, ax = plt.subplots(figsize=(20, 6))\n",
        "    ax.plot(test_obs, label=\"Observation\", lw=3, linestyle=\"--\")\n",
        "    ax.plot(test_preds, label=\"Prediction\")\n",
        "    ax.set_title(f\"{model_name}, Fold: {fold_idx} - Observation vs Prediction\")\n",
        "    ax.legend()\n",
        "    ax.set_xlabel(\"Day\")\n",
        "    ax.grid(True)\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Helper Functions for Creating Sequences & DataLoaders\n",
        "# -------------------------------------------------------------------\n",
        "def create_time_series_sequences(features, targets, sequence_length):\n",
        "    if isinstance(features, (pd.DataFrame, pd.Series)):\n",
        "        features = features.values\n",
        "    if isinstance(targets, pd.Series):\n",
        "        targets = targets.values\n",
        "    X_sequences, y_sequences = [], []\n",
        "    for i in range(len(features) - sequence_length):\n",
        "        X_sequences.append(features[i : i + sequence_length])\n",
        "        y_sequences.append(targets[i + sequence_length])\n",
        "    return np.array(X_sequences), np.array(y_sequences)\n",
        "\n",
        "\n",
        "def create_data_loader(X, y, batch_size=32, shuffle=True):\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
        "    dataset = TensorDataset(X_tensor, y_tensor)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "\n",
        "def create_transformer_input_sequences(features, targets, sequence_length):\n",
        "    return create_time_series_sequences(features, targets, sequence_length)\n",
        "\n",
        "\n",
        "def create_transformer_data_loader(X, y, batch_size=32, shuffle=True):\n",
        "    return create_data_loader(X, y, batch_size=batch_size, shuffle=shuffle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4ajFLB-QSS9"
      },
      "source": [
        "\n",
        "# --------------------------- Data Preparation & Time series split ---------------------------\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "eViHuqD7UsT2"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Updated Data Preparation Functions\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "def custom_time_series_split(df, n_splits, test_size):\n",
        "    X = df.drop(columns=[\"station_id\", \"TD\"], errors=\"ignore\")\n",
        "    y = df[\"TD\"].copy()\n",
        "\n",
        "    X_train_list, y_train_list = [], []\n",
        "    X_test_list, y_test_list = [], []\n",
        "    train_idx_list, test_idx_list = [], []\n",
        "\n",
        "    from sklearn.model_selection import TimeSeriesSplit\n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits, test_size=test_size)\n",
        "    for fold, (train_index, test_index) in enumerate(tscv.split(X)):\n",
        "        print(f\"Fold {fold+1}: TRAIN indices=[{train_index[0]}..{train_index[-1]}], TEST=[{test_index[0]}..{test_index[-1]}]\")\n",
        "        X_train = X.iloc[train_index]\n",
        "        y_train = y.iloc[train_index]\n",
        "        X_test  = X.iloc[test_index]\n",
        "        y_test  = y.iloc[test_index]\n",
        "\n",
        "        # Append the indices for later use\n",
        "        train_idx_list.append(train_index)\n",
        "        test_idx_list.append(test_index)\n",
        "\n",
        "        from sklearn.preprocessing import MinMaxScaler\n",
        "        scaler = MinMaxScaler()\n",
        "        scaler.fit(X_train)\n",
        "        X_train_scaled = scaler.transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "        X_train_list.append(X_train_scaled)\n",
        "        y_train_list.append(y_train)\n",
        "        X_test_list.append(X_test_scaled)\n",
        "        y_test_list.append(y_test)\n",
        "\n",
        "    # Return both the data splits and the indices splits\n",
        "    return X_train_list, y_train_list, X_test_list, y_test_list, train_idx_list, test_idx_list\n",
        "\n",
        "\n",
        "def prepare_data(df, n_splits, test_size_ratio):\n",
        "    \"\"\"\n",
        "    Prepare the data:\n",
        "      - Convert 'datetime' column to datetime objects.\n",
        "      - Sort by datetime.\n",
        "      - Normalize the 'RH' column.\n",
        "      - Replace invalid values and fill missing data.\n",
        "      - Optionally drop 'TW'.\n",
        "      - Set datetime as index.\n",
        "      - Compute test set size using a ratio and a heuristic.\n",
        "      - Split the data using custom_time_series_split.\n",
        "\n",
        "    Parameters:\n",
        "        df : DataFrame with a 'datetime' column.\n",
        "        n_splits (int): Number of splits.\n",
        "        test_size_ratio (float): Desired test set ratio.\n",
        "\n",
        "    Returns:\n",
        "        X_train_list, y_train_list, X_test_list, y_test_list for each fold.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df['datetime'] = pd.to_datetime(df['datetime'], utc=True, errors='coerce')\n",
        "    df = df.sort_values('datetime')\n",
        "\n",
        "    df[\"RH\"] = df[\"RH\"] / 100.0\n",
        "    df.replace(-9999.0, np.nan, inplace=True)\n",
        "\n",
        "    if 'TW' in df.columns:\n",
        "        df.drop(columns='TW', inplace=True)\n",
        "\n",
        "    for col in df.columns[df.isna().any()]:\n",
        "        df[col] = df[col].fillna(df[col].mean())\n",
        "\n",
        "    df.set_index('datetime', inplace=True)\n",
        "\n",
        "    test_size = int(len(df) * test_size_ratio)\n",
        "    max_test_size = len(df) // (n_splits * 2)\n",
        "    if test_size < max_test_size:\n",
        "        test_size = max_test_size\n",
        "\n",
        "    print(f\"Using test_size={test_size} ({(test_size/len(df))*100:.1f}% of dataset)\")\n",
        "    return custom_time_series_split(df, n_splits, test_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kL_Bi2kuQlNd"
      },
      "source": [
        "# --------------------------- Linear Models (Benchmark) ---------------------------\n",
        "\n",
        "## Note: keep the model with the best results when the Data is completed\n",
        "Keep the model that demonstrates the best balance of:\n",
        "\n",
        "- Generalization performance (lowest MAE/MSE, highest R² on unseen data),\n",
        "- Stability (consistent performance across folds), and\n",
        "- Interpretability (if needed, the simpler or sparser model might be preferred)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "0TPtp-uWQlYb"
      },
      "outputs": [],
      "source": [
        "# Linear Regression - No regularization\n",
        "# Formula: y = Xw + b\n",
        "def linear_regression_model(train_X, test_X, train_y, test_y, **kwargs):\n",
        "    print(\"\\n--- Training Linear Regression Model ---\")\n",
        "    model = LinearRegression(**kwargs)\n",
        "    return train_and_evaluate(model, \"Linear Regression\", train_X, test_X, train_y, test_y)\n",
        "\n",
        "# Ridge Regression - L2 regularization\n",
        "# Formula: Minimize ||y - Xw||^2 + alpha * ||w||^2\n",
        "def ridge_regression_model(train_X, test_X, train_y, test_y, **kwargs):\n",
        "    print(\"\\n--- Training Ridge Regression Model ---\")\n",
        "    model = Ridge(**kwargs)\n",
        "    return train_and_evaluate(model, \"Ridge Regression\", train_X, test_X, train_y, test_y)\n",
        "\n",
        "# Lasso Regressio - L1 regularization\n",
        "# Formula: Minimize ||y - Xw||^2 + alpha * ||w||_1\n",
        "def lasso_regression_model(train_X, test_X, train_y, test_y, **kwargs):\n",
        "    print(\"\\n--- Training Lasso Regression Model ---\")\n",
        "    model = Lasso(**kwargs)\n",
        "    return train_and_evaluate(model, \"Lasso Regression\", train_X, test_X, train_y, test_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaSPSxD_W4mp"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXqbqcNIQ6Sj"
      },
      "source": [
        "# --------------------------- Random Forest Model ---------------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "sGIU2R9LQ6If"
      },
      "outputs": [],
      "source": [
        "def random_forest_model(train_X, test_X, train_y, test_y, **kwargs):\n",
        "    print(\"\\n--- Training Random Forest Model ---\")\n",
        "    model = RandomForestRegressor(**kwargs)\n",
        "    return train_and_evaluate(model, \"Random Forest\", train_X, test_X, train_y, test_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyuoLqdKtTOo"
      },
      "source": [
        "# --------------------------- LSTM Model ---------------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "F9-YkkYGtTH0"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# LSTM Model & Training Functions\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "class WeatherForecastLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_units=128, lstm_layers=2, dropout_rate=0.3):\n",
        "        super(WeatherForecastLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_units,\n",
        "            num_layers=lstm_layers,\n",
        "            dropout=dropout_rate if lstm_layers > 1 else 0,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_units * 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        final_output = self.fc(lstm_out[:, -1, :])\n",
        "        return final_output\n",
        "\n",
        "def train_lstm_model(model, train_loader, val_loader, loss_function, optimizer,\n",
        "                     epochs=50, patience=10, save_path=\"best_lstm_model.pth\"):\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
        "                                                           patience=5, factor=0.5, verbose=True)\n",
        "    model.to(device)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0.0\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(batch_X)\n",
        "            loss = loss_function(predictions, batch_y)\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            total_train_loss += loss.item()\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        model.eval()\n",
        "        total_val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in val_loader:\n",
        "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "                predictions = model(batch_X)\n",
        "                loss = loss_function(predictions, batch_y)\n",
        "                total_val_loss += loss.item()\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        scheduler.step(avg_val_loss)\n",
        "        print(f\"LSTM Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(\"New best LSTM model saved.\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping for LSTM triggered.\")\n",
        "                break\n",
        "    model.load_state_dict(torch.load(save_path))\n",
        "    return model\n",
        "\n",
        "def train_and_evaluate_lstm_model(train_X, test_X, train_y, test_y, sequence_length=144, **kwargs):\n",
        "    batch_size = kwargs.get(\"batch_size\", 16)\n",
        "    hidden_units = kwargs.get(\"hidden_units\", 128)\n",
        "    lstm_layers = kwargs.get(\"lstm_layers\", 2)\n",
        "    dropout_rate = kwargs.get(\"dropout_rate\", 0.3)\n",
        "    learning_rate = kwargs.get(\"learning_rate\", 0.001)\n",
        "    epochs = kwargs.get(\"epochs\", 50)\n",
        "\n",
        "    train_X_seq, train_y_seq = create_time_series_sequences(train_X, train_y, sequence_length)\n",
        "    test_X_seq, test_y_seq = create_time_series_sequences(test_X, test_y, sequence_length)\n",
        "\n",
        "    split_index = int(len(train_X_seq) * 0.8)\n",
        "    train_loader = create_data_loader(train_X_seq[:split_index], train_y_seq[:split_index], batch_size=batch_size, shuffle=True)\n",
        "    val_loader   = create_data_loader(train_X_seq[split_index:], train_y_seq[split_index:], batch_size=batch_size, shuffle=False)\n",
        "    test_loader  = create_data_loader(test_X_seq, test_y_seq, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    input_dim = train_X_seq.shape[2]\n",
        "    model = WeatherForecastLSTM(input_dim, hidden_units, lstm_layers, dropout_rate)\n",
        "\n",
        "    loss_function = nn.MSELoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    model = train_lstm_model(model, train_loader, val_loader, loss_function, optimizer,\n",
        "                             epochs=epochs, patience=10, save_path=\"best_lstm_model.pth\")\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for batch_X, _ in test_loader:\n",
        "            batch_X = batch_X.to(device)\n",
        "            batch_preds = model(batch_X)\n",
        "            predictions.extend(batch_preds.squeeze().cpu().numpy())\n",
        "    return evaluate_model(\"LSTM\", test_y[sequence_length:], predictions) + (predictions,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcnR4nAUaXlW"
      },
      "source": [
        "# --------------------------- Transformer Model ---------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "G4xLi-_6aXg2"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Transformer Model & Training Functions\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  # shape (1, max_len, d_model)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class TimeSeriesTransformer(nn.Module):\n",
        "    def __init__(self, input_dim, d_model=128, num_heads=4, num_layers=2, dropout=0.1):\n",
        "        super(TimeSeriesTransformer, self).__init__()\n",
        "        self.input_layer = nn.Linear(input_dim, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=num_heads,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.output_layer = nn.Linear(d_model, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_layer(x)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = self.output_layer(x[:, -1, :])\n",
        "        return x\n",
        "\n",
        "def train_transformer(model, train_loader, val_loader, loss_function, optimizer,\n",
        "                      epochs=50, patience=10, save_path=\"best_transformer.pth\"):\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
        "                                                           patience=5, factor=0.5, verbose=True)\n",
        "    model.to(device)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0.0\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(batch_X)\n",
        "            loss = loss_function(predictions, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_train_loss += loss.item()\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        model.eval()\n",
        "        total_val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in val_loader:\n",
        "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "                predictions = model(batch_X)\n",
        "                loss = loss_function(predictions, batch_y)\n",
        "                total_val_loss += loss.item()\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        scheduler.step(avg_val_loss)\n",
        "        print(f\"Transformer Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(\"New best Transformer model saved.\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping for Transformer triggered.\")\n",
        "                break\n",
        "    model.load_state_dict(torch.load(save_path))\n",
        "    return model\n",
        "\n",
        "def train_and_evaluate_transformer_model(train_X, test_X, train_y, test_y, sequence_length=144, **kwargs):\n",
        "    batch_size = kwargs.get(\"batch_size\", 16)\n",
        "    d_model = kwargs.get(\"d_model\", 128)\n",
        "    num_heads = kwargs.get(\"num_heads\", 4)\n",
        "    num_layers = kwargs.get(\"num_layers\", 2)\n",
        "    dropout = kwargs.get(\"dropout\", 0.1)\n",
        "    learning_rate = kwargs.get(\"learning_rate\", 0.001)\n",
        "    epochs = kwargs.get(\"epochs\", 50)\n",
        "\n",
        "    train_X_seq, train_y_seq = create_transformer_input_sequences(train_X, train_y, sequence_length)\n",
        "    test_X_seq, test_y_seq = create_transformer_input_sequences(test_X, test_y, sequence_length)\n",
        "\n",
        "    split_index = int(len(train_X_seq) * 0.8)\n",
        "    train_loader = create_transformer_data_loader(train_X_seq[:split_index], train_y_seq[:split_index], batch_size=batch_size, shuffle=True)\n",
        "    val_loader   = create_transformer_data_loader(train_X_seq[split_index:], train_y_seq[split_index:], batch_size=batch_size, shuffle=False)\n",
        "    test_loader  = create_transformer_data_loader(test_X_seq, test_y_seq, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    input_dim = train_X_seq.shape[2]\n",
        "    model = TimeSeriesTransformer(input_dim, d_model, num_heads, num_layers, dropout)\n",
        "\n",
        "    loss_function = nn.MSELoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    model = train_transformer(model, train_loader, val_loader, loss_function, optimizer,\n",
        "                              epochs=epochs, patience=10, save_path=\"best_transformer.pth\")\n",
        "\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for batch_X, _ in test_loader:\n",
        "            batch_X = batch_X.to(device)\n",
        "            batch_preds = model(batch_X)\n",
        "            predictions.extend(batch_preds.squeeze().cpu().numpy())\n",
        "    return evaluate_model(\"Transformer\", test_y[sequence_length:], predictions) + (predictions,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l92IPPwNjiRW"
      },
      "source": [
        "# --------------------------- Graph Neural Network Model ---------------------------\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "0lYZaBzojhd4"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Graph Neural Network (GNN) Model & Training Functions\n",
        "# -------------------------------------------------------------------\n",
        "def create_graph_data_with_window(train_X, train_y, test_X, test_y, window=2):\n",
        "    \"\"\"\n",
        "    Build graph data using a temporal window.\n",
        "    Each node is connected to the next 'window' nodes.\n",
        "    This is a simple modification to provide richer temporal connectivity.\n",
        "    \"\"\"\n",
        "    import torch\n",
        "    from torch_geometric.data import Data\n",
        "\n",
        "    # Convert inputs to numpy arrays\n",
        "    train_X = np.array(train_X)\n",
        "    test_X  = np.array(test_X)\n",
        "    train_y = np.array(train_y)\n",
        "    test_y  = np.array(test_y)\n",
        "\n",
        "    def build_edges(num_nodes, window):\n",
        "        edges = []\n",
        "        for i in range(num_nodes):\n",
        "            for j in range(1, window+1):\n",
        "                if i+j < num_nodes:\n",
        "                    edges.append([i, i+j])\n",
        "        return edges\n",
        "\n",
        "    train_edge_list = build_edges(len(train_X), window)\n",
        "    test_edge_list  = build_edges(len(test_X), window)\n",
        "\n",
        "    print(f\"Created {len(train_edge_list)} edges for training with window={window}.\")\n",
        "    print(f\"Created {len(test_edge_list)} edges for testing with window={window}.\")\n",
        "\n",
        "    train_edge_index = torch.tensor(train_edge_list, dtype=torch.long).t().contiguous() if train_edge_list else torch.empty((2,0), dtype=torch.long)\n",
        "    test_edge_index  = torch.tensor(test_edge_list, dtype=torch.long).t().contiguous() if test_edge_list else torch.empty((2,0), dtype=torch.long)\n",
        "\n",
        "    train_X_tensor = torch.tensor(train_X, dtype=torch.float32)\n",
        "    test_X_tensor  = torch.tensor(test_X, dtype=torch.float32)\n",
        "    train_y_tensor = torch.tensor(train_y, dtype=torch.float32)\n",
        "    test_y_tensor  = torch.tensor(test_y, dtype=torch.float32)\n",
        "\n",
        "    train_data = Data(x=train_X_tensor, edge_index=train_edge_index, y=train_y_tensor)\n",
        "    test_data  = Data(x=test_X_tensor, edge_index=test_edge_index, y=test_y_tensor)\n",
        "\n",
        "    return train_data, test_data\n",
        "\n",
        "# first ver\n",
        "# class GraphNeuralNetwork(nn.Module):\n",
        "#     def __init__(self, num_features, hidden_dim=64, dropout=0.5):\n",
        "#         super(GraphNeuralNetwork, self).__init__()\n",
        "#         self.conv1 = GCNConv(num_features, hidden_dim)\n",
        "#         self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
        "#         self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "#         self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
        "#         self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
        "#         self.bn3 = nn.BatchNorm1d(hidden_dim)\n",
        "#         self.fc    = nn.Linear(hidden_dim, 1)\n",
        "#         self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "#     def forward(self, x, edge_index):\n",
        "#         x = self.conv1(x, edge_index)\n",
        "#         x = self.bn1(x)\n",
        "#         x = F.relu(x)\n",
        "#         x = self.dropout(x)\n",
        "#         x = self.conv2(x, edge_index)\n",
        "#         x = self.bn2(x)\n",
        "#         x = F.relu(x)\n",
        "#         x = self.dropout(x)\n",
        "#         x = self.conv3(x, edge_index)\n",
        "#         x = self.bn3(x)\n",
        "#         x = F.relu(x)\n",
        "#         x = self.dropout(x)\n",
        "#         x = self.fc(x)\n",
        "#         return x.squeeze()\n",
        "\n",
        "# second ver\n",
        "class GraphNeuralNetwork(nn.Module):\n",
        "    def __init__(self, num_features, hidden_dim=64, dropout=0.5, negative_slope=0.01):\n",
        "        super(GraphNeuralNetwork, self).__init__()\n",
        "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.bn3 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.negative_slope = negative_slope  # Parameter for LeakyReLU\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.leaky_relu(x, negative_slope=self.negative_slope)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.leaky_relu(x, negative_slope=self.negative_slope)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = self.bn3(x)\n",
        "        x = F.leaky_relu(x, negative_slope=self.negative_slope)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x.squeeze()\n",
        "\n",
        "\n",
        "def train_gnn(model, train_data, epochs=50, lr=0.001):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_function = nn.MSELoss()\n",
        "    model.to(device)\n",
        "    train_data = train_data.to(device)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(train_data.x, train_data.edge_index)\n",
        "        loss = loss_function(predictions, train_data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f\"GNN Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "    return model\n",
        "\n",
        "def evaluate_gnn_final(model, test_data, test_y):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = model(test_data.x.to(device), test_data.edge_index.to(device)).cpu().numpy()\n",
        "    mae_val  = mean_absolute_error(test_y, predictions)\n",
        "    mse_val  = mean_squared_error(test_y, predictions)\n",
        "    r2_val   = r2_score(test_y, predictions)\n",
        "    mape_val = median_absolute_percentage_error(test_y, predictions)\n",
        "    print(\"GNN Performance:\")\n",
        "    print(f\"  MAE: {mae_val:.4f}\")\n",
        "    print(f\"  MSE: {mse_val:.4f}\")\n",
        "    print(f\"  R2 Score: {r2_val:.4f}\")\n",
        "    print(f\"  MdAPE: {mape_val:.4f}%\\n\")\n",
        "    return mae_val, mse_val, r2_val, mape_val, predictions\n",
        "\n",
        "def train_and_evaluate_gnn_model(train_X, test_X, train_y, test_y, station_ids=None, **kwargs):\n",
        "    hidden_dim = kwargs.get(\"hidden_dim\", 64)\n",
        "    dropout = kwargs.get(\"dropout\", 0.5)\n",
        "    learning_rate = kwargs.get(\"learning_rate\", 0.001)\n",
        "    epochs = kwargs.get(\"epochs\", 50)\n",
        "\n",
        "    print(\"\\n--- Preparing Graph Data ---\")\n",
        "    train_data, test_data = create_graph_data_with_window(train_X, train_y, test_X, test_y, window=5)\n",
        "    print(\"\\n--- Training GNN ---\")\n",
        "    model = GraphNeuralNetwork(num_features=train_X.shape[1], hidden_dim=hidden_dim, dropout=dropout)\n",
        "    model = train_gnn(model, train_data, epochs=epochs, lr=learning_rate)\n",
        "    print(\"\\n--- Evaluating GNN ---\")\n",
        "    mae_val, mse_val, r2_val, mape_val, predictions = evaluate_gnn_final(model, test_data, test_y)\n",
        "    return mae_val, mse_val, r2_val, mape_val, predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfNHBbZCp4J3"
      },
      "source": [
        "# --------------------------- EDA ---------------------------\n",
        "# EVALUATION METRICS:\n",
        "## **1. Mean Absolute Error (MAE)**\n",
        "- Measures the average absolute difference between predictions and actual values.\n",
        "- **Ideal:** Lower is better (0 = perfect predictions).  \n",
        "---\n",
        "\n",
        "## **2. Mean Squared Error (MSE)**\n",
        "- Calculates the average squared differences, penalizing larger errors.\n",
        "- **Ideal:** Lower is better (0 = perfect predictions).  \n",
        "---\n",
        "\n",
        "## **3. R² Score (Coefficient of Determination)**\n",
        "- Indicates how well the model explains variance in the target variable.\n",
        "- **Ideal:** Higher is better (**1** = perfect fit, **0** = baseline, negative = poor fit).  \n",
        "---\n",
        "\n",
        "## **4. Median Absolute Percentage Error (MdAPE)**\n",
        "- Measures the average percentage error, providing a relative error measure.\n",
        "- **Ideal:** Lower is better (0% indicates perfect predictions).  \n",
        "---\n",
        "\n",
        "# **Ideal Metric Summary**\n",
        "\n",
        "| **Metric**   | **Goal**            | **Description**                   |\n",
        "|-------------|---------------------|-----------------------------------|\n",
        "| **MAE**     | Lower is better     | Average absolute error           |\n",
        "| **MSE**     | Lower is better     | Penalizes large errors more      |\n",
        "| **R² Score** | Higher is better   | Variance explained by the model  |\n",
        "| **MdAPE**    | Lower is better     | Median Absolute Percentage Error |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "WsuBite0EI41"
      },
      "outputs": [],
      "source": [
        "# Tomers code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_-54cAWMUQq"
      },
      "source": [
        "#**Recommended `sequence_length` Based on Your Goal**\n",
        "The dataset consists of **10-minute interval weather data** spanning **25 years**. The `sequence_length` determines how many past time steps the model should use to make predictions.\n",
        "\n",
        "| **Goal** | **Recommended `sequence_length`** | **Explanation** |\n",
        "|----------|---------------------------------|---------------------------------------------------------------|\n",
        "| Predict next 1-6 hours | **6 - 36** | Uses recent short-term fluctuations (e.g., sudden weather changes). |\n",
        "| Predict next 12 hours | **72** | Captures half-day variations (e.g., temperature and humidity shifts). |\n",
        "| Predict tomorrow’s weather | **144** (1 day) | Accounts for daily cycles (e.g., daytime vs. nighttime temperature). |\n",
        "| Predict the next 3 days | **432** (3 days) | Captures short-term trends, good for local weather forecasts. |\n",
        "| Capture weekly patterns | **1008** (7 days) | Considers weekly trends (e.g., regular temperature changes, weekly patterns). |\n",
        "| Predict the next 10 days | **1440** (10 days) | Captures longer short-term trends, useful for medium-range forecasting. |\n",
        "| Predict the next 14 days | **2016** (14 days) | Captures bi-weekly trends, useful for extended forecasts. |\n",
        "| Predict monthly variations | **4320** (30 days) | Considers longer-term weather fluctuations, such as seasonal shifts. |\n",
        "| **Capture seasonal changes** | **12960** (3 months) | Tracks full-season trends like winter, summer, monsoon. |\n",
        "| **Capture yearly variations** | **52560** (1 year) | Useful for long-term climate trend analysis. |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "NH3mTUa2OEWR"
      },
      "outputs": [],
      "source": [
        "def get_sequence_length(forecast_horizon):\n",
        "    \"\"\"\n",
        "    Determines the appropriate sequence_length based on the forecast horizon.\n",
        "\n",
        "    Args:\n",
        "        forecast_horizon (str): Forecasting time frame (\"hourly\", \"daily\", \"weekly\", \"seasonal\", \"yearly\").\n",
        "\n",
        "    Returns:\n",
        "        int: Number of past time steps to use as input.\n",
        "    \"\"\"\n",
        "    sequence_lengths = {\n",
        "        \"hourly\": 6,        # 1-hour (6 x 10-minute intervals)\n",
        "        \"short\": 144,       # 1-day (144 x 10-minute intervals)\n",
        "        \"3-day\": 432,       # 3 days (3 × 144)\n",
        "        \"weekly\": 1008,     # 7 days (7 × 144)\n",
        "        \"10-day\": 1440,     # 10 days (10 × 144)\n",
        "        \"bi-weekly\": 2016,  # 14 days (14 × 144)\n",
        "        \"monthly\": 4320,    # 30 days (30 × 144)\n",
        "        \"seasonal\": 12960,  # 90 days (3 months × 30 days × 144)\n",
        "        \"yearly\": 52560     # 1 year (365 × 144)\n",
        "    }\n",
        "\n",
        "    if forecast_horizon not in sequence_lengths:\n",
        "        raise ValueError(f\"Invalid forecast_horizon: {forecast_horizon}. Choose from {list(sequence_lengths.keys())}.\")\n",
        "\n",
        "    return sequence_lengths[forecast_horizon]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4gMiRkrRH3Z"
      },
      "source": [
        "# --------------------------- Main ---------------------------\n",
        "\n",
        "this part in the code -Main function- is responseble to load the data, train all models, and compare their performances.\n",
        "    \n",
        "This part includes the activation and evaluation of all the models defined above:\n",
        "\n",
        "  * Linear Regression, Ridge Regression, and Lasso Regression (Linear Models)\n",
        "  * K-Nearest Neighbors (KNN)\n",
        "  * Decision Tree and Random Forest (Tree-Based Models)\n",
        "  * Deep learning - Neural Network  \n",
        "    - Multi-Layer Perceptron (MLP) - using ReLU-activation and backpropagation\n",
        "    - Long Short-Term Memory (LSTM)\n",
        "    - and more\n",
        "\n",
        "    \n",
        "After training, the models' performances are evaluated and compared using MAE, MSE, and R2 Score.\n",
        "The results are visualized for a clearer comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "HlDo363N-46b"
      },
      "outputs": [],
      "source": [
        "# keep for later - df = pd.read_csv('/content/drive/MyDrive/Course70938/earthML/finalProject/Data/IMS/allStations_10min.csv', on_bad_lines=\"skip\")  # Noam's dataset path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "QMP9dcihRRL-"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Main function\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "def main(n_splits=3, test_size_ratio=0.1, forecast_horizon=\"short\"):\n",
        "    \"\"\"\n",
        "    Main function to:\n",
        "    - Load and preprocess data\n",
        "    - Perform time-series split\n",
        "    - Train multiple models (Linear, LSTM, Transformer, GNN, etc.)\n",
        "    - Evaluate performance\n",
        "    - Plot results\n",
        "\n",
        "    Args:\n",
        "        n_splits (int): Number of splits for TimeSeries cross-validation.\n",
        "        test_size_ratio (float): Ratio of dataset to use for testing (e.g., 0.1 = 10%).\n",
        "        forecast_horizon (str): \"short\" for short-term forecasting (1 day), \"long\" for long-term forecasting (14 days).\n",
        "    \"\"\"\n",
        "    # Load Dataset\n",
        "    df = pd.read_csv('/content/drive/MyDrive/earth/earthML/finalProject/Data/IMS/allStations_10min_TEMP.csv', on_bad_lines=\"skip\")# Tomer path\n",
        "    # df = pd.read_csv('/content/drive/MyDrive/Course70938/earthML/finalProject/Data/IMS/allStations_10min_TEMP.csv', on_bad_lines=\"skip\")  # test path\n",
        "    print(f\"Initial shape of data: {df.shape}\")\n",
        "\n",
        "### delete later:\n",
        "    # print(df.columns)\n",
        "    # print(df[\"TD\"].describe())\n",
        "    # plt.hist(df[\"TD\"], bins=50)\n",
        "    # plt.xlabel(\"TD\")\n",
        "    # plt.ylabel(\"Frequency\")\n",
        "    # plt.title(\"Distribution of TD\")\n",
        "    # plt.show()\n",
        "### delete later:\n",
        "\n",
        "    # Determine sequence_length based on forecast_horizon\n",
        "    sequence_length = get_sequence_length(forecast_horizon)\n",
        "\n",
        "    # Automatically Calculate Test Size\n",
        "    test_size = int(len(df) * test_size_ratio)\n",
        "    if test_size < sequence_length:\n",
        "        print(f\"Warning: test_size={test_size} is too small for sequence_length={sequence_length}. Adjusting...\")\n",
        "        test_size = sequence_length + 10000  # Adjust to a safe margin\n",
        "    else:\n",
        "        print(f\"Using test_size={test_size} ({test_size_ratio * 100}% of dataset) with sequence_length={sequence_length} for forecast_horizon='{forecast_horizon}'.\")\n",
        "\n",
        "    # Prepare Data\n",
        "    X_train_list, y_train_list, X_test_list, y_test_list, train_idx_list, test_idx_list = prepare_data(df, n_splits, test_size_ratio)\n",
        "    print(f\"Prepared {len(X_train_list)} folds of data.\")\n",
        "\n",
        "    # Dictionary containing hyperparameters for each model\n",
        "    model_hyperparams = {\n",
        "        \"Linear Regression\": {},\n",
        "        \"Ridge Regression\": {\"alpha\": 1.0},\n",
        "        \"Lasso Regression\": {\"alpha\": 0.1},\n",
        "        \"Random Forest\": {\"n_estimators\": 200, \"max_depth\": 50, \"min_samples_split\": 4, \"random_state\": 0},\n",
        "        \"LSTM\": {\n",
        "            \"hidden_units\": 128,\n",
        "            \"lstm_layers\": 3,\n",
        "            \"dropout_rate\": 0.3,\n",
        "            \"batch_size\": 256,\n",
        "            \"learning_rate\": 0.001,\n",
        "            \"epochs\": 50\n",
        "        },\n",
        "        \"Transformer\": {\n",
        "            \"d_model\": 128,\n",
        "            \"num_heads\": 4,\n",
        "            \"num_layers\": 2,\n",
        "            \"dropout\": 0.2,\n",
        "            \"batch_size\": 256,\n",
        "            \"learning_rate\": 0.001,\n",
        "            \"epochs\": 50\n",
        "        },\n",
        "        \"Graph Neural Network\": {\n",
        "            \"hidden_dim\": 256,\n",
        "            \"batch_size\": 32,\n",
        "            \"learning_rate\": 0.005,\n",
        "            \"epochs\": 100\n",
        "        }\n",
        "    }\n",
        "        # 18.1\n",
        "        #     \"Graph Neural Network\": {\n",
        "        #     \"hidden_dim\": 256,\n",
        "        #     \"batch_size\": 32,\n",
        "        #     \"learning_rate\": 0.01,\n",
        "        #     \"epochs\": 100\n",
        "        # }\n",
        "\n",
        "    # Initialize results dictionary\n",
        "    results = {\n",
        "        'Model': [],\n",
        "        'Fold': [],\n",
        "        'MAE': [],\n",
        "        'MSE': [],\n",
        "        'R2 Score': [],\n",
        "        'MdAPE': []\n",
        "    }\n",
        "\n",
        "    # Define models\n",
        "    model_funcs = {\n",
        "        # 'Linear Regression': linear_regression_model,\n",
        "        # 'Ridge Regression': ridge_regression_model,\n",
        "        # 'Lasso Regression': lasso_regression_model,\n",
        "        # 'Random Forest': random_forest_model,\n",
        "        # 'LSTM': train_and_evaluate_lstm_model,\n",
        "        # 'Transformer': train_and_evaluate_transformer_model,\n",
        "        'Graph Neural Network': train_and_evaluate_gnn_model\n",
        "    }\n",
        "\n",
        "    # Train and evaluate each model for each fold\n",
        "    for fold_idx in range(len(X_train_list)):\n",
        "        print(f\"\\n================= Fold {fold_idx + 1} =================\")\n",
        "        train_X, test_X = X_train_list[fold_idx], X_test_list[fold_idx]\n",
        "        train_y, test_y = y_train_list[fold_idx], y_test_list[fold_idx]\n",
        "\n",
        "        for model_name, model_func in model_funcs.items():\n",
        "            print(f\"\\n--- {model_name} ---\")\n",
        "            try:\n",
        "                hyperparams = model_hyperparams.get(model_name, {})\n",
        "\n",
        "                if model_name in {'LSTM', 'Transformer'}:\n",
        "                    mae, mse, r2, mdape_val, prediction = model_func(train_X, test_X, train_y, test_y,\n",
        "                                                                      sequence_length=sequence_length, **hyperparams)\n",
        "                    start_index = test_y.index[sequence_length:] if hasattr(test_y, 'index') else range(sequence_length, len(test_y))\n",
        "                    test_preds = pd.Series(prediction, index=test_y.index[start_index[0]:] if hasattr(test_y, 'index') else None)\n",
        "                    test_obs_sliced = test_y.loc[start_index] if hasattr(test_y, 'index') else test_y\n",
        "                    plot_series(test_obs_sliced, test_preds, model_name, fold_idx + 1)\n",
        "                elif model_name == \"Graph Neural Network\":\n",
        "                    # Option A: Use station-based grouping (if you want to use station IDs or Stations Coordinations)\n",
        "                    # train_station_ids = df[\"station_id\"].iloc[train_idx_list[fold_idx]].values\n",
        "                    # test_station_ids  = df[\"station_id\"].iloc[test_idx_list[fold_idx]].values\n",
        "                    # unique_edge = (train_station_ids, test_station_ids)\n",
        "\n",
        "                    # Option B: Use sequential edges with a temporal window (recommended to test improved connectivity)\n",
        "                    unique_edge = None  # Use sequential edges\n",
        "\n",
        "                    mae, mse, r2, mdape_val, predictions = model_func(train_X, test_X, train_y, test_y, unique_edge, **hyperparams)\n",
        "                    test_preds = pd.Series(predictions, index=test_y.index)\n",
        "                    plot_series(test_y, test_preds, model_name, fold_idx+1)\n",
        "                else:\n",
        "                    mae, mse, r2, mdape_val, prediction = model_func(train_X, test_X, train_y, test_y, **hyperparams)\n",
        "                    test_preds = pd.Series(prediction, index=test_y.index)\n",
        "                    plot_series(test_y, test_preds, model_name, fold_idx + 1)\n",
        "\n",
        "                results['Model'].append(model_name)\n",
        "                results['Fold'].append(fold_idx + 1)\n",
        "                results['MAE'].append(mae)\n",
        "                results['MSE'].append(mse)\n",
        "                results['R2 Score'].append(r2)\n",
        "                results['MdAPE'].append(mdape_val)\n",
        "            except Exception as e:\n",
        "                print(f\"Error with {model_name} on Fold {fold_idx + 1}: {e}\")\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "    print(\"\\n--- Model Performance Summary ---\")\n",
        "    print(results_df)\n",
        "    plot_model_performance(results_df)\n",
        "    plot_model_performance1(results_df)\n",
        "\n",
        "\n",
        "def plot_model_performance(results_df):\n",
        "    \"\"\"\n",
        "    Plots a comparison of model performance metrics.\n",
        "    \"\"\"\n",
        "    metrics_to_plot = ['MAE', 'MSE', 'R2 Score', 'MdAPE']\n",
        "    for metric in metrics_to_plot:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        pivot_df = results_df.pivot(index='Model', columns='Fold', values=metric)\n",
        "        pivot_df.plot(kind='bar', title=f\"Model Comparison by {metric}\", figsize=(10, 6))\n",
        "        plt.ylabel(metric)\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.legend(title=\"Fold\", loc=\"upper right\")\n",
        "        plt.show()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_model_performance1(results_df):\n",
        "    \"\"\"\n",
        "    Plots a comparison of model performance metrics.\n",
        "\n",
        "    Improvements:\n",
        "    - Better readability with distinct titles and labels.\n",
        "    - Improved layout for clarity.\n",
        "    - Performance trend visualization.\n",
        "    \"\"\"\n",
        "    metrics_to_plot = ['MAE', 'MSE', 'R2 Score', 'MdAPE']\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))  # Arrange in 2x2 grid\n",
        "    axes = axes.flatten()  # Flatten for easy iteration\n",
        "\n",
        "    # Assign colors for models\n",
        "    palette = sns.color_palette(\"tab10\", len(results_df[\"Model\"].unique()))\n",
        "\n",
        "    for idx, metric in enumerate(metrics_to_plot):\n",
        "        ax = axes[idx]\n",
        "        sns.barplot(data=results_df, x='Model', y=metric, hue='Fold', ax=ax, palette=palette)\n",
        "\n",
        "        # Formatting\n",
        "        ax.set_title(f\"Comparison of {metric} across Models\", fontsize=14)\n",
        "        ax.set_xlabel(\"Model\", fontsize=12)\n",
        "        ax.set_ylabel(metric, fontsize=12)\n",
        "        ax.legend(title=\"Fold\", loc=\"best\")\n",
        "        ax.tick_params(axis='x', rotation=30)  # Rotate x-axis labels for better visibility\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "O4W9LnjFrzVV",
        "outputId": "7e3da7e5-0d9c-4f95-fbec-51a5f1e7632e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Course70938/earthML/finalProject/Data/IMS/allStations_10min_TEMP.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-e1cb82aebc6b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforecast_horizon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"weekly\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-61-4cc69c6cfe2c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(n_splits, test_size_ratio, forecast_horizon)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Load Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# df = pd.read_csv('/content/drive/MyDrive/earth/earthML/finalProject/Data/IMS/allStations_10min_TEMP.csv', on_bad_lines=\"skip\")# Tomer path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Course70938/earthML/finalProject/Data/IMS/allStations_10min_TEMP.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_bad_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"skip\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# test path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Initial shape of data: {df.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Course70938/earthML/finalProject/Data/IMS/allStations_10min_TEMP.csv'"
          ]
        }
      ],
      "source": [
        "# ------------------------------------------------------------------- #\n",
        "#                             Entry Point\n",
        "# ------------------------------------------------------------------- #\n",
        "\"\"\"\n",
        "A short recap on 'forecast_horizon':\n",
        "    \"hourly\": 6,        # 1-hour (6 x 10-minute intervals)\n",
        "    \"short\": 144,       # 1-day (144 x 10-minute intervals)\n",
        "    \"3-day\": 432,       # 3 days (3 × 144)\n",
        "    \"weekly\": 1008,     # 7 days (7 × 144)\n",
        "    \"10-day\": 1440,     # 10 days (10 × 144)\n",
        "    \"bi-weekly\": 2016,  # 14 days (14 × 144)\n",
        "    \"monthly\": 4320,    # 30 days (30 × 144)\n",
        "    \"seasonal\": 12960,  # 90 days (3 months × 30 days × 144)\n",
        "    \"yearly\": 52560     # 1 year (365 × 144)\n",
        "\"\"\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(n_splits=3, test_size_ratio=0.1, forecast_horizon=\"weekly\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "oT9VjYMeY7sM",
        "g4ajFLB-QSS9",
        "kL_Bi2kuQlNd",
        "SXqbqcNIQ6Sj",
        "YyuoLqdKtTOo",
        "FcnR4nAUaXlW",
        "UfNHBbZCp4J3",
        "4_-54cAWMUQq"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}